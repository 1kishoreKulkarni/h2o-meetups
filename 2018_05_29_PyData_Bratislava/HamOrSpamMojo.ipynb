{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import HashingTF, RegexTokenizer, StopWordsRemover, IDF\n",
    "from pysparkling import *\n",
    "from pysparkling.ml import ColumnPruner, H2ODeepLearning\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.16.225.190:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x109b77350>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check Spark is ready\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to H2O server at http://172.16.225.190:54321... successful.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td>H2O cluster uptime:</td>\n",
       "<td>06 secs</td></tr>\n",
       "<tr><td>H2O cluster timezone:</td>\n",
       "<td>Europe/Bratislava</td></tr>\n",
       "<tr><td>H2O data parsing timezone:</td>\n",
       "<td>UTC</td></tr>\n",
       "<tr><td>H2O cluster version:</td>\n",
       "<td>3.18.0.11</td></tr>\n",
       "<tr><td>H2O cluster version age:</td>\n",
       "<td>5 days </td></tr>\n",
       "<tr><td>H2O cluster name:</td>\n",
       "<td>sparkling-water-kuba_local-1527615165506</td></tr>\n",
       "<tr><td>H2O cluster total nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O cluster free memory:</td>\n",
       "<td>846 Mb</td></tr>\n",
       "<tr><td>H2O cluster total cores:</td>\n",
       "<td>8</td></tr>\n",
       "<tr><td>H2O cluster allowed cores:</td>\n",
       "<td>8</td></tr>\n",
       "<tr><td>H2O cluster status:</td>\n",
       "<td>accepting new members, healthy</td></tr>\n",
       "<tr><td>H2O connection url:</td>\n",
       "<td>http://172.16.225.190:54321</td></tr>\n",
       "<tr><td>H2O connection proxy:</td>\n",
       "<td>None</td></tr>\n",
       "<tr><td>H2O internal security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>H2O API Extensions:</td>\n",
       "<td>XGBoost, Algos, AutoML, Core V3, Core V4</td></tr>\n",
       "<tr><td>Python version:</td>\n",
       "<td>2.7.14 final</td></tr></table></div>"
      ],
      "text/plain": [
       "--------------------------  ----------------------------------------\n",
       "H2O cluster uptime:         06 secs\n",
       "H2O cluster timezone:       Europe/Bratislava\n",
       "H2O data parsing timezone:  UTC\n",
       "H2O cluster version:        3.18.0.11\n",
       "H2O cluster version age:    5 days\n",
       "H2O cluster name:           sparkling-water-kuba_local-1527615165506\n",
       "H2O cluster total nodes:    1\n",
       "H2O cluster free memory:    846 Mb\n",
       "H2O cluster total cores:    8\n",
       "H2O cluster allowed cores:  8\n",
       "H2O cluster status:         accepting new members, healthy\n",
       "H2O connection url:         http://172.16.225.190:54321\n",
       "H2O connection proxy:\n",
       "H2O internal security:      False\n",
       "H2O API Extensions:         XGBoost, Algos, AutoML, Core V3, Core V4\n",
       "Python version:             2.7.14 final\n",
       "--------------------------  ----------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sparkling Water Context:\n",
      " * H2O name: sparkling-water-kuba_local-1527615165506\n",
      " * cluster size: 1\n",
      " * list of used nodes:\n",
      "  (executorId, host, port)\n",
      "  ------------------------\n",
      "  (driver,172.16.225.190,54321)\n",
      "  ------------------------\n",
      "\n",
      "  Open H2O Flow in browser: http://172.16.225.190:54321 (CMD + click in Mac OSX)\n",
      "\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# Initialize H2O Context\n",
    "hc = H2OContext.getOrCreate(spark)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is just helper function returning path to data-files\n",
    "def _locate(file_name):\n",
    "        return \"sparkling-water/examples/smalldata/\" + file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This method loads the data, perform some basic filtering and create Spark's dataframe\n",
    "def load():\n",
    "    row_rdd = spark.sparkContext.textFile(_locate(\"smsData.txt\")).map(lambda x: x.split(\"\\t\", 1)).filter(lambda r: r[0].strip())\n",
    "    return spark.createDataFrame(row_rdd, [\"label\", \"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Define the pipeline stages\n",
    "##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenize the messages\n",
    "tokenizer = RegexTokenizer(inputCol=\"text\",\n",
    "                           outputCol=\"words\",\n",
    "                           minTokenLength=3,\n",
    "                           gaps=False,\n",
    "                           pattern=\"[a-zA-Z]+\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove ignored words\n",
    "stopWordsRemover = StopWordsRemover(inputCol=tokenizer.getOutputCol(),\n",
    "                                    outputCol=\"filtered\",\n",
    "                                    stopWords=[\"the\", \"a\", \"\", \"in\", \"on\", \"at\", \"as\", \"not\", \"for\"],\n",
    "                                    caseSensitive=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hash the words\n",
    "hashingTF = HashingTF(inputCol=stopWordsRemover.getOutputCol(),\n",
    "                      outputCol=\"wordToIndex\",\n",
    "                      numFeatures=1 << 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create inverse document frequencies model\n",
    "idf = IDF(inputCol=hashingTF.getOutputCol(),\n",
    "          outputCol=\"tf_idf\",\n",
    "          minDocFreq=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create H2ODeepLearning model\n",
    "dl = H2ODeepLearning(epochs=10,\n",
    "                     l1=0.001,\n",
    "                     l2=0.0,\n",
    "                     hidden=[200, 200],\n",
    "                     featuresCols=[idf.getOutputCol()],\n",
    "                     predictionCol=\"label\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove all helper columns\n",
    "colPruner = ColumnPruner(columns=[\n",
    "        idf.getOutputCol(),\n",
    "        hashingTF.getOutputCol(),\n",
    "        stopWordsRemover.getOutputCol(),\n",
    "        tokenizer.getOutputCol()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Create the pipeline by defining all the stages\n",
    "pipeline = Pipeline(stages=[tokenizer, stopWordsRemover, hashingTF, idf, dl, colPruner])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train the pipeline model\n",
    "data = load()\n",
    "model = pipeline.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Make predictions on unlabeled data\n",
    "## Spam detector\n",
    "##\n",
    "def isSpam(smsText, model, h2oContext, hamThreshold = 0.5):\n",
    "    smsTextDF = spark.createDataFrame([(smsText,)], [\"text\"]) # create one element tuple\n",
    "    prediction = model.transform(smsTextDF)\n",
    "    return prediction.first()[\"prediction_output\"][\"p0\"] > hamThreshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(isSpam(\"Hello, party tonight after our Bratislava meetup?\", model, hc))\n",
    "\n",
    "print(isSpam(\"We tried to contact you re your reply to our offer of a Video Handset? 750 anytime any networks mins? UNLIMITED TEXT?\", model, hc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
