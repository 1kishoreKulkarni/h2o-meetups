{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import requests_cache\n",
    "from requests_oauthlib import OAuth1Session\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Requests Cache is a great way to save on API calls\n",
    "# http://requests-cache.readthedocs.io/\n",
    "requests_cache.install_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Link to your own JSON credentials file\n",
    "credentials = json.loads(open('oauth.txt').read())\n",
    "\n",
    "twitter = OAuth1Session(client_key=credentials[\"client_key\"],\n",
    "                    client_secret=credentials[\"client_secret\"],\n",
    "                    resource_owner_key=credentials[\"resource_owner_key\"],\n",
    "                    resource_owner_secret=credentials[\"resource_owner_secret\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "candidates = [\"realdonaldtrump\", \"hillaryclinton\"]\n",
    "\n",
    "for candidate in candidates:\n",
    "\n",
    "    base_url = \"https://api.twitter.com/1.1/statuses/user_timeline.json?screen_name=%s&count=100\" % candidate\n",
    "    tweets = pd.DataFrame(columns=[\"id\", \"text\", \"created_at\", \"favorite_count\", 'retweet_count']) # We add to this DF\n",
    "    maxid = str(twitter.get(base_url).json()[0]['id']) # Start with the most recent Tweet id\n",
    "\n",
    "    for j in range(0, 35): # 35, because 35 * 100 Tweets per call is just more than the 3000 we're able to access\n",
    "        r = twitter.get(base_url+\"&max_id=\"+maxid)\n",
    "        for i in r.json():\n",
    "            # Add to our Tweets DataFrame\n",
    "            tweets.loc[tweets.shape[0]] = [str(i[\"id\"]), i[\"text\"], i[\"created_at\"], i[\"favorite_count\"], i['retweet_count']]\n",
    "            maxid = str(i[\"id\"]) # Take the oldest Tweet id for use in getting the next 100 Tweets\n",
    "    \n",
    "    tweets = tweets.drop_duplicates(subset=[\"id\"])\n",
    "    tweets.to_csv(\"%s_tweets.csv\" % candidate, encoding='utf-8') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
